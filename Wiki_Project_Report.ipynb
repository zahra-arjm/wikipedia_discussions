{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQ1SFErVjbI3DYRBUt78uo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zahra-arjm/wikipedia_discussions/blob/improve_code/Wiki_Project_Report.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a report of a mini-project for visualizing wikipedia conversations on summer 2022. The data was gathered by Christine De Kock and Andreas Vlachos. The code was written by me ([Zahra Arjmandi Lari](mailto:z.arjmandi@gmail.com)). Part of the credit for the ideas on \"how to visualize\" goes back to Tom Stafford."
      ],
      "metadata": {
        "id": "iHPw7Uq9zXvD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Story In Short\n",
        "Wikipedia pages have a [talk/discussion page](https://en.wikipedia.org/wiki/Help:Talk_pages) where editors discuss how to improve a page. In some occasions, the editors have different opinions on some topics and the talk page will be tagged as \"dispute\" by editors. In some cases the editors resolve the disputes on their own; in other cases the disputes are \"escalated\" and needs mediation.\n",
        "Wikipedia suggests its users to use hierarchy of disagreement, proposed by Paul Graham [1](#refs) to resolve disputes constructively. Christine De Kock and Andreas Vlachos have annotated ~200 conversation from wikipedia talk pages categorized as \"dispute\". They have labeled each part of the conversation (utterance) accordingly which consist of ~4000 utterance [2](#refs). Part of the labels are on the basis of Graham's proposed hierarchy of disagreement [1](#refs) which addresses the \"rebuttal tactics\". Graham suggest 7 levels for disagreements\n",
        "which (starts from name-calling at the bottom or DH0 to refuting the central point or DH7). The other part of labels are called \"resolution tactics\" and attempts to promote\n",
        "understanding and consensus. See table 1 [ADD TABLE]. For more information see [2](#refs).\n",
        "\n",
        "### Our Question\n",
        "What we wanted to know was \"what is the difference in how conversation flows in \"escalated\" versus \"non-escalated\" discussions?\"\n",
        "### What We Did In Summary\n",
        "I divided data into escalated and non-escalated conversations. For each utterance, picked one or multiple label(s). Then, built the transition matrix and visualized the graphs; each label served as a node and the transition probability between each two nodes, became the strength of the edges.\n",
        "First, we wanted to know whether escalated conversation lack higher order DHs or not. I drawed the graph such that in case of multilabel utterance, the higher label is shown (for more information see [1](#refs)). Our graphs showed that escalated discussion do have at least the same amount of higher order DHs.[reference to related cells]\n",
        "Then we thought that maybe the higher order DHs are accompanied with lower order DHs in escalated conversations. Potentially, this could neutralize the effect of using higher order resolution techniques. [reference to related cells] Based on the graphs, it seems like in 'escalated' conversations lower order DHs were used more!\n",
        "In the end, we had the idea of dividing transitions such that each label has the chance to be part of the graph. It shows that less resolusional tactics were used in 'escalated' conversations, compared to 'non-escalated' ones. It also shows that 'rebuttal' tactics did not encourage 'resolusional' tactics (such as 'Asking questions' \"providing a clarification\", or \"suggesting a compromise\"\n",
        ") in 'escalated' conversations versus 'non-escalated' ones.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Dck4-ECRvd_n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###What happens in the code\n"
      ],
      "metadata": {
        "id": "4VJhu3AY1PRn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 1: Downloading dataset and import libraries**\n",
        "\n",
        "Start by downlaoding/uploading raw data file (frequency_data.json)"
      ],
      "metadata": {
        "id": "OJw06o-T15cJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "0f90NhyYpY-l",
        "outputId": "629834b7-e71c-447f-c0af-80403aabb65d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bab0d1a0-7b39-48d9-becd-2cc261667417\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-bab0d1a0-7b39-48d9-becd-2cc261667417\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving frequency_data.json to frequency_data.json\n"
          ]
        }
      ],
      "source": [
        "# import data\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "# upload \"frequency_data.json\"\n",
        "uploaded = files.upload()\n",
        "\n",
        "#download the raw data from a git repo\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import libraries. Numpy for handling matrices and json for handling the raw data file (opening and reading .json format). Pyvis and networkx are the two libraries for visulaizing graphs. I have used IPython to help in displaying the graphs (.html format) inside this notebook."
      ],
      "metadata": {
        "id": "hxk-ypV92GIy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4s4avQZFNwY",
        "outputId": "6c2cedfc-2633-4f92-f250-88af21199798"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyvis\n",
            "  Downloading pyvis-0.3.2-py3-none-any.whl (756 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/756.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.3/756.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ipython>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from pyvis) (7.34.0)\n",
            "Requirement already satisfied: jinja2>=2.9.6 in /usr/local/lib/python3.10/dist-packages (from pyvis) (3.1.2)\n",
            "Requirement already satisfied: jsonpickle>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from pyvis) (3.0.1)\n",
            "Requirement already satisfied: networkx>=1.11 in /usr/local/lib/python3.10/dist-packages (from pyvis) (3.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython>=5.3.0->pyvis)\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (3.0.38)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (2.14.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (4.8.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.9.6->pyvis) (2.1.3)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.3.0->pyvis) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=5.3.0->pyvis) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.3.0->pyvis) (0.2.6)\n",
            "Installing collected packages: jedi, pyvis\n",
            "Successfully installed jedi-0.18.2 pyvis-0.3.2\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#import needed libraries\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "# install pyvis\n",
        "# pyvis is a network visualization library based on Networkx\n",
        "#which allow graphs to be interactive. You can also save them as html\n",
        "!pip install pyvis\n",
        "from pyvis.network import Network\n",
        "\n",
        "# to be able to display output graph which is in html format\n",
        "from IPython.core.display import display, HTML"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **Part 2: Data cleaning and wrangling**\n",
        "\n",
        "Next, open the data file and load it. Let's have a look at how the data format:"
      ],
      "metadata": {
        "id": "7KXOrOUWD-33"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8G95o36BT6H",
        "outputId": "8b1258b5-162c-47ce-9001-c778910b736c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'utt_labels': [['Coordinating edits'],\n",
              "   ['Contextualisation'],\n",
              "   ['Providing clarification'],\n",
              "   ['DH5: Counterargument'],\n",
              "   ['DH5: Counterargument'],\n",
              "   ['DH4: Repeated argument'],\n",
              "   ['DH4: Repeated argument'],\n",
              "   ['Suggesting a compromise', 'Coordinating edits'],\n",
              "   ['Coordinating edits'],\n",
              "   ['Coordinating edits'],\n",
              "   ['Coordinating edits'],\n",
              "   ['DH1: Ad hominem/ad argument'],\n",
              "   ['DH4: Repeated argument'],\n",
              "   ['DH4: Stating your stance'],\n",
              "   ['Other'],\n",
              "   ['Coordinating edits'],\n",
              "   ['Coordinating edits'],\n",
              "   ['DH5: Counterargument'],\n",
              "   ['DH4: Stating your stance'],\n",
              "   ['DH4: Stating your stance'],\n",
              "   ['DH4: Stating your stance'],\n",
              "   ['DH4: Repeated argument'],\n",
              "   ['DH4: Stating your stance']],\n",
              "  'escalation': 0},\n",
              " {'utt_labels': [['Contextualisation'],\n",
              "   ['Coordinating edits'],\n",
              "   ['Providing clarification'],\n",
              "   ['DH1: Ad hominem/ad argument', 'DH5: Counterargument'],\n",
              "   ['DH5: Counterargument'],\n",
              "   ['DH5: Counterargument',\n",
              "    'DH0: Name calling/hostility',\n",
              "    'DH1: Ad hominem/ad argument'],\n",
              "   ['DH1: Ad hominem/ad argument', 'DH4: Stating your stance'],\n",
              "   ['DH1: Ad hominem/ad argument', 'DH3: Policing the discussion']],\n",
              "  'escalation': 1},\n",
              " {'utt_labels': [['Coordinating edits'],\n",
              "   ['Contextualisation'],\n",
              "   ['DH5: Counterargument'],\n",
              "   ['DH4: Stating your stance'],\n",
              "   ['DH4: Repeated argument'],\n",
              "   ['DH5: Counterargument'],\n",
              "   ['DH1: Ad hominem/ad argument'],\n",
              "   ['DH5: Counterargument', 'DH3: Policing the discussion'],\n",
              "   ['DH1: Ad hominem/ad argument'],\n",
              "   ['DH1: Ad hominem/ad argument', 'DH5: Counterargument'],\n",
              "   ['DH4: Stating your stance', 'Asking questions'],\n",
              "   ['Coordinating edits'],\n",
              "   ['DH4: Repeated argument'],\n",
              "   ['DH5: Counterargument'],\n",
              "   ['DH4: Stating your stance'],\n",
              "   ['Coordinating edits'],\n",
              "   ['DH5: Counterargument'],\n",
              "   ['DH4: Stating your stance'],\n",
              "   ['Suggesting a compromise'],\n",
              "   ['Conceding / recanting'],\n",
              "   ['Conceding / recanting'],\n",
              "   ['Conceding / recanting'],\n",
              "   ['Coordinating edits'],\n",
              "   ['Coordinating edits'],\n",
              "   ['DH5: Counterargument'],\n",
              "   ['DH4: Stating your stance'],\n",
              "   ['DH4: Stating your stance'],\n",
              "   ['DH6: Refutation']],\n",
              "  'escalation': 0},\n",
              " {'utt_labels': [['Contextualisation'],\n",
              "   ['Coordinating edits'],\n",
              "   ['DH6: Refutation'],\n",
              "   ['DH6: Refutation', 'Conceding / recanting'],\n",
              "   ['DH4: Repeated argument', 'Coordinating edits'],\n",
              "   ['DH6: Refutation'],\n",
              "   ['DH6: Refutation'],\n",
              "   ['DH4: Repeated argument', 'DH5: Counterargument'],\n",
              "   ['DH4: Repeated argument'],\n",
              "   ['DH4: Repeated argument'],\n",
              "   ['DH4: Repeated argument'],\n",
              "   ['DH4: Repeated argument'],\n",
              "   ['DH4: Repeated argument', 'DH1: Ad hominem/ad argument'],\n",
              "   ['DH5: Counterargument']],\n",
              "  'escalation': 1},\n",
              " {'utt_labels': [['Contextualisation'],\n",
              "   ['Coordinating edits'],\n",
              "   ['DH5: Counterargument'],\n",
              "   ['DH5: Counterargument'],\n",
              "   ['DH5: Counterargument'],\n",
              "   ['DH4: Stating your stance', 'DH4: Repeated argument'],\n",
              "   ['DH3: Policing the discussion', 'DH1: Ad hominem/ad argument'],\n",
              "   ['DH4: Repeated argument'],\n",
              "   ['DH1: Ad hominem/ad argument'],\n",
              "   ['DH4: Repeated argument']],\n",
              "  'escalation': 1}]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "\n",
        "# Opening JSON file\n",
        "raw_data = open('frequency_data.json')\n",
        "\n",
        "# returns JSON object as\n",
        "# a dictionary\n",
        "raw_data = json.load(raw_data)\n",
        "\n",
        "raw_data[0:5]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The raw_data is a list containing multiple dictionaries. Each dictionary has two keys:\n",
        "\n",
        "*   escalation: which shows the type of coversation (either escalated with value 1 or non-escalated with value 0)\n",
        "*   utt_labels: which is shows each utterance's label. Its value is a list of lists containing each utterance label(s). These lists are in the same order that happend in the conversation.\n",
        "\n"
      ],
      "metadata": {
        "id": "1Na2OU7B3A-6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get an idea of the labels' frequency, I built a dictionary contining all the labels and their frequnecies:"
      ],
      "metadata": {
        "id": "3S_cR9t372-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#count frequency of each label in a dictionary\n",
        "\n",
        "from collections import Counter\n",
        "super_dict_labels = {}\n",
        "for i in range(len(raw_data)):\n",
        "\n",
        "\n",
        "  dict_list = ([dict(Counter(x)) for x in raw_data[i]['utt_labels']])\n",
        "  for item in dict_list:\n",
        "    for k, v in item.items():\n",
        "      if k in super_dict_labels.keys():\n",
        "        super_dict_labels[k] += v\n",
        "      else:\n",
        "        super_dict_labels[k] = v\n",
        "super_dict_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJ28czvQ8IW5",
        "outputId": "6e31789f-ddc3-4e00-eb0f-c3015323b986"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Coordinating edits': 972,\n",
              " 'Contextualisation': 208,\n",
              " 'Providing clarification': 144,\n",
              " 'DH5: Counterargument': 988,\n",
              " 'DH4: Repeated argument': 572,\n",
              " 'Suggesting a compromise': 89,\n",
              " 'DH1: Ad hominem/ad argument': 566,\n",
              " 'DH4: Stating your stance': 433,\n",
              " 'Other': 25,\n",
              " 'DH0: Name calling/hostility': 65,\n",
              " 'DH3: Policing the discussion': 249,\n",
              " 'Asking questions': 92,\n",
              " 'Conceding / recanting': 59,\n",
              " 'DH6: Refutation': 300,\n",
              " 'DH2: Attempted derailing/off-topic': 65,\n",
              " 'DH-1: Bailing out': 37,\n",
              " 'DH7: Refuting the central point': 19,\n",
              " \"I don't know\": 7,\n",
              " 'Other: Quote': 1,\n",
              " 'DH5: Counterargument with new evidence / reasoning': 8,\n",
              " \"DH6: Refutation of opponent's argument (with evidence or reasoning)\": 5,\n",
              " 'DH1: Attacks to the person or argument': 9,\n",
              " 'DH4: Stating your stance without evidence or reasoning': 5,\n",
              " 'DH2: Attempted derailing / off-topic comments': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are multiple labels. All DH labels are in format of DH[a digit] exept for 'DH-1: Bailing out'. Let's change that:"
      ],
      "metadata": {
        "id": "VNV79bpx8NC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# change label \"DH-1: Bailing out\" to formal format \"DH1: Bailing out\"\n",
        "for conversation in raw_data:\n",
        "  for labels in conversation['utt_labels']:\n",
        "    #check if DH- is there\n",
        "    if \"DH-1: Bailing out\" in labels:\n",
        "      idx = labels.index(\"DH-1: Bailing out\")\n",
        "      labels[idx] = \"DH1: Bailing out\"\n",
        "\n",
        "\n",
        "#count frequency of each label in a dictionary after changing DH-1 to DH1\n",
        "\n",
        "super_dict_labels = {}\n",
        "for i in range(len(raw_data)):\n",
        "\n",
        "\n",
        "  dict_list = ([dict(Counter(x)) for x in raw_data[i]['utt_labels']])\n",
        "  for item in dict_list:\n",
        "    for k, v in item.items():\n",
        "      if k in super_dict_labels.keys():\n",
        "        super_dict_labels[k] += v\n",
        "      else:\n",
        "        super_dict_labels[k] = v\n",
        "super_dict_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFOq7F2N2sIz",
        "outputId": "00a96d0d-17e4-4280-a963-a9adb1fd26e6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Coordinating edits': 972,\n",
              " 'Contextualisation': 208,\n",
              " 'Providing clarification': 144,\n",
              " 'DH5: Counterargument': 988,\n",
              " 'DH4: Repeated argument': 572,\n",
              " 'Suggesting a compromise': 89,\n",
              " 'DH1: Ad hominem/ad argument': 566,\n",
              " 'DH4: Stating your stance': 433,\n",
              " 'Other': 25,\n",
              " 'DH0: Name calling/hostility': 65,\n",
              " 'DH3: Policing the discussion': 249,\n",
              " 'Asking questions': 92,\n",
              " 'Conceding / recanting': 59,\n",
              " 'DH6: Refutation': 300,\n",
              " 'DH2: Attempted derailing/off-topic': 65,\n",
              " 'DH1: Bailing out': 37,\n",
              " 'DH7: Refuting the central point': 19,\n",
              " \"I don't know\": 7,\n",
              " 'Other: Quote': 1,\n",
              " 'DH5: Counterargument with new evidence / reasoning': 8,\n",
              " \"DH6: Refutation of opponent's argument (with evidence or reasoning)\": 5,\n",
              " 'DH1: Attacks to the person or argument': 9,\n",
              " 'DH4: Stating your stance without evidence or reasoning': 5,\n",
              " 'DH2: Attempted derailing / off-topic comments': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To build a transition matrix, I needed to simplify the multi-label utterances. The 'pick_label' function gets the list of labels and the method, and outputs the final label(s).\n",
        "I needed to either reduce multi-labels to one label or distribute the transition from i to i+1 th utterance between all the labels; such that each pair of label is involved equally.\n",
        "the function 'pick_label' accepts three methods:\n",
        "\n",
        "\n",
        "*   all: the output will be the same as the input. It gives back all the labels\n",
        "*   max: if the labels contain a DH label, the output will be the highest DH. Otherwise, the output is the label with the highest frequency\n",
        "\n",
        "*   min: if the labels contain a DH label, the output will be the lowest DH. Otherwise, the output is the label with the highest frequency\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{1}{\\text{(no of labels in ith utterance)} \\text{(no labels in (i+1)th utterance)}}\n",
        "\\end{align}\n",
        "\n"
      ],
      "metadata": {
        "id": "GNtG_VTL9xoC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "UxVjih9CqFhn"
      },
      "outputs": [],
      "source": [
        "\n",
        "# a function to pick one of the lables if there are multiple labels available for a part of an utter\n",
        "def pick_label(list_of_labels, method):\n",
        "  '''\n",
        "  This function picks one of the lables in case there are multiple labels\n",
        "   available.\n",
        "  In case both DH and non-DH labels are availabe it picks the DHs.\n",
        "  In case all of the labels are non-DH it picks the one with the highest frequency.\n",
        "\n",
        "  Args:\n",
        "  list_of_labels (list): labels for part of the utterance\n",
        "  method (string): if max or min, picks the highest or lowest DH. If all gives back all labels\n",
        "  '''\n",
        "\n",
        "  # first check no of labels or if method is 'all'\n",
        "  if len(list_of_labels) == 1 or method == 'all':\n",
        "    final_label = list_of_labels\n",
        "  else: #case of multiple labels\n",
        "    #check how many DHs are present\n",
        "    count_DH = 0\n",
        "    for item in list_of_labels:\n",
        "      if item[0:2] == 'DH':\n",
        "        count_DH += 1\n",
        "    if count_DH == 0:\n",
        "      #pick the one with the highest frequency\n",
        "      label_freqs = [super_dict_labels[x] for x in list_of_labels]\n",
        "      final_label = [list(super_dict_labels.keys())[list(super_dict_labels.values()).index(max(label_freqs))]]\n",
        "    elif count_DH == 1:\n",
        "      # pick the DH\n",
        "      final_label = [i for i in list_of_labels if i.startswith('DH')]\n",
        "    else:\n",
        "\n",
        "      #extract DH_no s\n",
        "      DH_no = [int(i[2]) for i in list_of_labels if i.startswith('DH')]\n",
        "      # select the DH based on the method specified\n",
        "      if method == 'min':\n",
        "        final_label = [i for i in list_of_labels\n",
        "                       if (i.startswith('DH') and int(i[2]) == min(DH_no))]\n",
        "      elif method == 'max':\n",
        "        final_label = [i for i in list_of_labels\n",
        "                       if (i.startswith('DH') and int(i[2]) == max(DH_no))]\n",
        "\n",
        "  return final_label\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, build the transition matrix. The matrix contains all the labels plus a label called 'end'. The last list in each 'utter_label' is followed by 'end' (or end of the conversation).\n",
        "Here, we can change the method of extracting labels in 'pick_label' function by changing 'method' variable.\n",
        "The 'transition_all' variable is a 3-dimensional array. On the first layer, we have the transitions between every two labels, for the non-escalated conversations (or label 0 on escalation), and on the second layer, we have the same thing for escalated conversations. I created three versions of transition matrix to try differnt methods of picking labels on the data."
      ],
      "metadata": {
        "id": "HsvL3k-CDiT-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Uj7xScmtpY29"
      },
      "outputs": [],
      "source": [
        "# create 3 np arrays to keep transitions for esc and non-esc conversations\n",
        "# 24 unique labels + 1 row/col for \"end\"\n",
        "#it's a 25*25*2 matrix. Layer 0 is for non_esc and 1 is for escelated conversations\n",
        "transition_all = np.zeros((len(super_dict_labels)+1, len(super_dict_labels)+1, 2), float)\n",
        "transition_max = np.zeros_like(transition_all)\n",
        "transition_min = np.zeros_like(transition_all)\n",
        "\n",
        "# put the list of labels into labels_long variable\n",
        "labels_long = list(super_dict_labels.keys())\n",
        "#add 'end' label to the list of labels\n",
        "labels_long.append('end')\n",
        "#choose which method to apply\n",
        "methods = ['all', 'max', 'min']\n",
        "# fill out the transition matrix\n",
        "for conversation in raw_data:\n",
        "  for idx_labels in range(len(conversation['utt_labels'])):\n",
        "\n",
        "    #loop through methods\n",
        "    for method in methods:\n",
        "      # find from and to labels\n",
        "      from_labels = pick_label(conversation['utt_labels'][idx_labels], method)\n",
        "      len_from = len(from_labels)\n",
        "\n",
        "      # two for loops in case we wanted to more than 1 label:\n",
        "      for from_label in from_labels:\n",
        "        #find index of the label in the list of all labels\n",
        "        from_idx = labels_long.index(from_label)\n",
        "\n",
        "        if idx_labels == len(conversation['utt_labels'])-1:\n",
        "          #if it's the last part of conversation the to_label is 'end'\n",
        "          to_idx = 24\n",
        "          if method == 'all':\n",
        "            transition_all[from_idx,to_idx,int(conversation['escalation'])] += 1/(len_from*len_to)\n",
        "          elif method == 'min':\n",
        "            transition_min[from_idx,to_idx,int(conversation['escalation'])] += 1/(len_from*len_to)\n",
        "          else:\n",
        "            transition_max[from_idx,to_idx,int(conversation['escalation'])] += 1/(len_from*len_to)\n",
        "        else:\n",
        "          to_labels = pick_label(conversation['utt_labels'][idx_labels+1], method)\n",
        "          len_to = len(to_labels)\n",
        "          for to_label in to_labels:\n",
        "            #find index of the label in the list of all labels\n",
        "            to_idx = labels_long.index(to_label)\n",
        "            #add transition to transition matrix\n",
        "            if method == 'all':\n",
        "              transition_all[from_idx,to_idx,int(conversation['escalation'])] += 1/(len_from*len_to)\n",
        "            elif method == 'min':\n",
        "              transition_min[from_idx,to_idx,int(conversation['escalation'])] += 1/(len_from*len_to)\n",
        "            else:\n",
        "              transition_max[from_idx,to_idx,int(conversation['escalation'])] += 1/(len_from*len_to)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "View long labels and then shorten them. Later, I will use short labels for node identification in the graph."
      ],
      "metadata": {
        "id": "5LXm6L1y1d_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# view long labels\n",
        "print(labels_long)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Of7bPbo009_w",
        "outputId": "b927dc42-0084-49f9-c59a-27a30a29e15e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Coordinating edits', 'Contextualisation', 'Providing clarification', 'DH5: Counterargument', 'DH4: Repeated argument', 'Suggesting a compromise', 'DH1: Ad hominem/ad argument', 'DH4: Stating your stance', 'Other', 'DH0: Name calling/hostility', 'DH3: Policing the discussion', 'Asking questions', 'Conceding / recanting', 'DH6: Refutation', 'DH2: Attempted derailing/off-topic', 'DH1: Bailing out', 'DH7: Refuting the central point', \"I don't know\", 'Other: Quote', 'DH5: Counterargument with new evidence / reasoning', \"DH6: Refutation of opponent's argument (with evidence or reasoning)\", 'DH1: Attacks to the person or argument', 'DH4: Stating your stance without evidence or reasoning', 'DH2: Attempted derailing / off-topic comments', 'end']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#short labels\n",
        "labels_short = ['Coords',\n",
        " 'Context',\n",
        " 'clarify',\n",
        " 'DH5',\n",
        " 'DH4:Repeat',\n",
        " 'Suggest',\n",
        " 'DH1:Ad',\n",
        " 'DH4:stance',\n",
        " 'Other',\n",
        " 'DH0',\n",
        " 'DH3',\n",
        " 'Ask',\n",
        " 'Conced',\n",
        " 'DH6',\n",
        " 'DH2',\n",
        " 'DH1:Bail',\n",
        " 'DH7',\n",
        " \"Don't know\",\n",
        " 'Other:Quote',\n",
        " 'DH5:arg with',\n",
        " \"DH6:with evid\",\n",
        " 'DH1:Attack',\n",
        " 'DH4:without evid',\n",
        " 'DH2:off',\n",
        " 'end']"
      ],
      "metadata": {
        "id": "ZJJiG9EK08Co"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiple categories of the same DH are among labels. Since we need to simplify the most, I merged them!\n",
        "Then, I created another filtered list of the labels and updated the 'trasiton_all' matrix accordingly."
      ],
      "metadata": {
        "id": "0X2aeSYQ1w0t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_mQUUBkw_X52"
      },
      "outputs": [],
      "source": [
        "# sum instances of similar DHs\n",
        "rep_DHs = ['DH1', 'DH2', 'DH4', 'DH5', 'DH6']\n",
        "#keep indices of duplicates\n",
        "to_be_rmvd_idx = []\n",
        "for DH in rep_DHs:\n",
        "\n",
        "  #find DH instances\n",
        "  idx = [labels_short.index(i) for i in labels_short if i.startswith(DH)]\n",
        "  to_be_rmvd_idx.extend(idx)\n",
        "  #sum similir DHs cols and keep it in desired shape\n",
        "  last_col = np.sum(transition_all[:,idx,:], axis = 1)[:,None,:]\n",
        "  #sum all DH4s rows and keep it in desired shape\n",
        "  last_row = np.sum(transition_all[idx,:,:], axis = 0)[None,:,:]\n",
        "  diagonal_elements = np.sum(transition_all[idx,idx,:],axis = 0)[None,None,:]\n",
        "  #add computed rows and columns to the transition_all array\n",
        "  transition_all = np.vstack((np.hstack((transition_all, last_col)), np.hstack((last_row, diagonal_elements))))\n",
        "\n",
        "  # method 'min'\n",
        "  #sum similir DHs cols and keep it in desired shape\n",
        "  last_col = np.sum(transition_min[:,idx,:], axis = 1)[:,None,:]\n",
        "  #sum all DH4s rows and keep it in desired shape\n",
        "  last_row = np.sum(transition_min[idx,:,:], axis = 0)[None,:,:]\n",
        "  diagonal_elements = np.sum(transition_min[idx,idx,:],axis = 0)[None,None,:]\n",
        "  transition_min = np.vstack((np.hstack((transition_min, last_col)), np.hstack((last_row, diagonal_elements))))\n",
        "\n",
        "  # method 'max'\n",
        "  #sum similir DHs cols and keep it in desired shape\n",
        "  last_col = np.sum(transition_max[:,idx,:], axis = 1)[:,None,:]\n",
        "  #sum all DH4s rows and keep it in desired shape\n",
        "  last_row = np.sum(transition_max[idx,:,:], axis = 0)[None,:,:]\n",
        "  diagonal_elements = np.sum(transition_max[idx,idx,:],axis = 0)[None,None,:]\n",
        "  transition_max = np.vstack((np.hstack((transition_max, last_col)), np.hstack((last_row, diagonal_elements))))\n",
        "#append careated labels to label list\n",
        "labels_short.extend(rep_DHs)\n",
        "labels_long.extend(rep_DHs)\n",
        "\n",
        "#create a new list of lables and remove previous duplicates\n",
        "labels_short_filtered = [label for i, label in enumerate(labels_short) if i not in to_be_rmvd_idx]\n",
        "labels_long_filtered = [label for i, label in enumerate(labels_long) if i not in to_be_rmvd_idx]\n",
        "\n",
        "#filter transition matrix for to be removed indices\n",
        "transition_all = np.delete(transition_all, to_be_rmvd_idx, axis = 0)\n",
        "transition_all = np.delete(transition_all, to_be_rmvd_idx, axis = 1)\n",
        "\n",
        "transition_min = np.delete(transition_min, to_be_rmvd_idx, axis = 0)\n",
        "transition_min = np.delete(transition_min, to_be_rmvd_idx, axis = 1)\n",
        "\n",
        "transition_max = np.delete(transition_max, to_be_rmvd_idx, axis = 0)\n",
        "transition_max = np.delete(transition_max, to_be_rmvd_idx, axis = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see the total number of transitions in each layer (escalated and non-escalated conversations)"
      ],
      "metadata": {
        "id": "N2ZwjeGC4JXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(np.sum(transition_max, axis=0), axis=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJkez2BK4cCF",
        "outputId": "0481e44b-fa16-42cd-a589-e2bcaaa5153e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1928., 1827.])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have ~2000 transition in each layer. To simplify, I merged labels with low inward transitions (because label 'end' does not have any outward transition). I summed over each column over both layers.\n",
        "After filtering, I gathered all the filtered labels into label 'Other'. Since there was an 'Other' label in the given labels, first, I checked if 'Other' label has survived filtering! To make the code simpler I have used three different cells for each method."
      ],
      "metadata": {
        "id": "pwkDja475VIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#filter the columns with less than a threshold of the inward transitions\n",
        "threshold_in = 40 # overall 4000 transitions. 40 is one percent\n",
        "\n",
        "# find index of other in label list\n",
        "other_idx = labels_short.index('Other')\n",
        "\n",
        "def update_filter_and_transtion_matrix(transition_matrix, other_idx, threshold_in):\n",
        "  #find filter indices\n",
        "  filter = (np.sum(np.sum(transition_matrix, axis=0), axis=1)) < threshold_in\n",
        "  #check if \"other\" label has survived!\n",
        "  if not filter[other_idx]: #if it's survived on its own\n",
        "    #sum all filtered labels into other column/row\n",
        "    #and add it with 'Other' col\n",
        "    transition_matrix[:,other_idx,:] += np.sum(transition_matrix[:,filter,:], axis = 1)\n",
        "    transition_matrix[other_idx,:,:] += np.sum(transition_matrix[filter,:,:], axis = 0)\n",
        "  else: #if 'Other' transitions was lower than the threshold on its own and therefore was removed\n",
        "    #sum filtered labels col and keep it in desired shape\n",
        "    last_col = np.sum(transition_matrix[:,filter,:], axis = 1)[:,None,:]\n",
        "    diagonal_elements = np.sum(transition_matrix[filter,filter,:],axis = 0)[None,None,:]\n",
        "    #check if col sum is above threshod after summation\n",
        "    if last_col.sum() + diagonal_elements.sum() > threshold_in:\n",
        "      #sum all removed rows and keep it in desired shape\n",
        "      last_row = np.sum(transition_matrix[filter,:,:], axis = 0)[None,:,:]\n",
        "      #add 'Other' sum to transition matrix\n",
        "      transition_matrix = np.vstack((np.hstack((transition_matrix, last_col)), np.hstack((last_row, diagonal_elements))))\n",
        "\n",
        "  # update filter based on updated transition matrix\n",
        "  filter = (np.sum(np.sum(transition_matrix, axis=0), axis=1)) > threshold_in\n",
        "\n",
        "  return filter\n",
        "\n",
        "\n",
        "def extract_flatten_transtions(transition_matrix, fliter):\n",
        "  return transition_matrix[filter,:,:][:,filter,:]\n",
        "\n",
        "\n",
        "def prepare_label_list(filter, labels_long_filtered, labels_short_filtered, transition_flat):\n",
        "  #filter labels list accordingnly\n",
        "  labels_long_flat = [item for (item, cond) in zip(labels_long_filtered, filter) if cond]\n",
        "  labels_short_flat = [item for (item, cond) in zip(labels_short_filtered, filter) if cond]\n",
        "\n",
        "  #check if Other has survived this time\n",
        "  if transition_flat.shape[0] != len(labels_long_flat): #'Other' label transition has survived after summation\n",
        "    labels_long_flat.append('Other')\n",
        "    labels_short_flat.append('Other')\n",
        "\n",
        "  return labels_long_flat, labels_short_flat"
      ],
      "metadata": {
        "id": "d2gQ3p5oga4P"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#find filter indices\n",
        "filter_all = (np.sum(np.sum(transition_all, axis=0), axis=1)) < threshold_in\n",
        "# find index of other in label list\n",
        "other_idx = labels_short.index('Other')\n",
        "filter_rtn, tr_fl = prepare_matrix_and_labels(filter_all, other_idx, transition_all, threshold_in)\n",
        "l_l_f_all, l_s_f_all = prepare_label_list(filter_rtn, labels_long_filtered, labels_short_filtered, tr_fl)"
      ],
      "metadata": {
        "id": "Hmub2dPgHjKQ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tr_fl == trans_flt_all"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krhIIohpIkTI",
        "outputId": "368e0151-0dd5-4987-d59a-89f2b7b522b7"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-38-048ed4b4ee5d>:1: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
            "  tr_fl == trans_flt_all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ywvSOUt1NSN8"
      },
      "outputs": [],
      "source": [
        "#filter the columns with less than a threshold of the inward transitions\n",
        "threshold_in = 40 # overall 4000 transitions. 40 is one percent\n",
        "\n",
        "#find filter indices\n",
        "filter = (np.sum(np.sum(transition_all, axis=0), axis=1)) < threshold_in\n",
        "\n",
        "# find index of other in label list\n",
        "other_idx = labels_short.index('Other')\n",
        "\n",
        "\n",
        "\n",
        "#check if \"other\" label has survived!\n",
        "if not filter[other_idx]: #if it's survived on its own\n",
        "\n",
        "  #sum all filtered labels into other column/row\n",
        "  #and add it with 'Other' col\n",
        "  transition_all[:,other_idx,:] += np.sum(transition_all[:,filter,:], axis = 1)\n",
        "  #sum all DH4s rows and keep it in desired shape\n",
        "  transition_all[other_idx,:,:] += np.sum(transition_all[filter,:,:], axis = 0)\n",
        "\n",
        "  # filter to find the above threshold ones\n",
        "  filter = (np.sum(np.sum(transition_all, axis=0), axis=1)) > threshold_in\n",
        "  trans_flt_all = transition_all[filter,:,:][:,filter,:]\n",
        "else: #if 'Other' transitions was lower than the threshold on its own and therefore was removed\n",
        "  #sum filtered labels col and keep it in desired shape\n",
        "  last_col = np.sum(transition_all[:,filter,:], axis = 1)[:,None,:]\n",
        "  diagonal_elements = np.sum(transition_all[filter,filter,:],axis = 0)[None,None,:]\n",
        "  #check if col sum is below threshod after summation\n",
        "  if last_col.sum() + diagonal_elements.sum() < threshold_in:\n",
        "    filter = (np.sum(np.sum(transition_all, axis=0), axis=1)) > threshold_in\n",
        "    trans_flt_all = transition_all[filter,:,:][:,filter,:]\n",
        "  else:\n",
        "    #sum all removed rows and keep it in desired shape\n",
        "    last_row = np.sum(transition_all[filter,:,:], axis = 0)[None,:,:]\n",
        "\n",
        "\n",
        "    #add 'Other' sum to transition matrix\n",
        "    transition_all = np.vstack((np.hstack((transition_all, last_col)), np.hstack((last_row, diagonal_elements))))\n",
        "      # filter to find the remaining ones\n",
        "    filter = (np.sum(np.sum(transition_all, axis=0), axis=1)) > threshold_in\n",
        "\n",
        "    trans_flt_all = transition_all[filter,:,:][:,filter,:]\n",
        "\n",
        "#filter labels list accordingnly\n",
        "labels_long_flt_all = [item for (item, cond) in zip(labels_long_filtered, filter) if cond]\n",
        "labels_short_flt_all = [item for (item, cond) in zip(labels_short_filtered, filter) if cond]\n",
        "\n",
        "#check if Other has survived this time\n",
        "if trans_flt_all.shape[0] != len(labels_long_flt_all): #'Other' label transition has survived after summation\n",
        "  labels_long_flt_all.append('Other')\n",
        "  labels_short_flt_all.append('Other')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#method 'min'\n",
        "\n",
        "#find filter indices\n",
        "filter = (np.sum(np.sum(transition_all, axis=0), axis=1)) < threshold_in\n",
        "\n",
        "# find index of other in label list\n",
        "other_idx = labels_short.index('Other')\n",
        "\n",
        "\n",
        "#check if \"other\" label has survived!\n",
        "if not filter[other_idx]: #if it's survived on its own\n",
        "\n",
        "  #sum all filtered labels into other column/row\n",
        "  #and add it with 'Other' col\n",
        "  transition_min[:,other_idx,:] += np.sum(transition_min[:,filter,:], axis = 1)\n",
        "  #sum all DH4s rows and keep it in desired shape\n",
        "  transition_min[other_idx,:,:] += np.sum(transition_min[filter,:,:], axis = 0)\n",
        "\n",
        "  # filter to find the above threshold ones\n",
        "  filter = (np.sum(np.sum(transition_min, axis=0), axis=1)) > threshold_in\n",
        "  trans_flt_min = transition_min[filter,:,:][:,filter,:]\n",
        "else: #if 'Other' transitions was lower than the threshold on its own and therefore was removed\n",
        "  #sum filtered labels col and keep it in desired shape\n",
        "  last_col = np.sum(transition_min[:,filter,:], axis = 1)[:,None,:]\n",
        "  diagonal_elements = np.sum(transition_min[filter,filter,:],axis = 0)[None,None,:]\n",
        "  #check if col sum is below threshod after summation\n",
        "  if last_col.sum() + diagonal_elements.sum() < threshold_in:\n",
        "    filter = (np.sum(np.sum(transition_min, axis=0), axis=1)) > threshold_in\n",
        "    trans_flt_min = transition_min[filter,:,:][:,filter,:]\n",
        "  else:\n",
        "    #sum all removed rows and keep it in desired shape\n",
        "    last_row = np.sum(transition_min[filter,:,:], axis = 0)[None,:,:]\n",
        "\n",
        "\n",
        "    #add 'Other' sum to transition matrix\n",
        "    transition_min = np.vstack((np.hstack((transition_min, last_col)), np.hstack((last_row, diagonal_elements))))\n",
        "      # filter to find the remaining ones\n",
        "    filter = (np.sum(np.sum(transition_min, axis=0), axis=1)) > threshold_in\n",
        "\n",
        "    trans_flt_min = transition_min[filter,:,:][:,filter,:]\n",
        "\n",
        "#filter labels list accordingnly\n",
        "labels_long_flt_min = [item for (item, cond) in zip(labels_long_filtered, filter) if cond]\n",
        "labels_short_flt_min = [item for (item, cond) in zip(labels_short_filtered, filter) if cond]\n",
        "\n",
        "#check if Other has survived this time\n",
        "if trans_flt_min.shape[0] != len(labels_long_flt_min): #'Other' label transition has survived after summation\n",
        "  labels_long_flt_min.append('Other')\n",
        "  labels_short_flt_min.append('Other')"
      ],
      "metadata": {
        "id": "LHgngzD2V38W"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#method 'max'\n",
        "\n",
        "#find filter indices\n",
        "filter = (np.sum(np.sum(transition_all, axis=0), axis=1)) < threshold_in\n",
        "\n",
        "# find index of other in label list\n",
        "other_idx = labels_short.index('Other')\n",
        "\n",
        "\n",
        "#check if \"other\" label has survived!\n",
        "if not filter[other_idx]: #if it's survived on its own\n",
        "\n",
        "  #sum all filtered labels into other column/row\n",
        "  #and add it with 'Other' col\n",
        "  transition_max[:,other_idx,:] += np.sum(transition_max[:,filter,:], axis = 1)\n",
        "  #sum all DH4s rows and keep it in desired shape\n",
        "  transition_max[other_idx,:,:] += np.sum(transition_max[filter,:,:], axis = 0)\n",
        "\n",
        "  # filter to find the above threshold ones\n",
        "  filter = (np.sum(np.sum(transition_max, axis=0), axis=1)) > threshold_in\n",
        "  trans_flt_max = transition_max[filter,:,:][:,filter,:]\n",
        "else: #if 'Other' transitions was lower than the threshold on its own and therefore was removed\n",
        "  #sum filtered labels col and keep it in desired shape\n",
        "  last_col = np.sum(transition_max[:,filter,:], axis = 1)[:,None,:]\n",
        "  diagonal_elements = np.sum(transition_max[filter,filter,:],axis = 0)[None,None,:]\n",
        "  #check if col sum is below threshod after summation\n",
        "  if last_col.sum() + diagonal_elements.sum() < threshold_in:\n",
        "    filter = (np.sum(np.sum(transition_max, axis=0), axis=1)) > threshold_in\n",
        "    trans_flt_max = transition_max[filter,:,:][:,filter,:]\n",
        "  else:\n",
        "    #sum all removed rows and keep it in desired shape\n",
        "    last_row = np.sum(transition_max[filter,:,:], axis = 0)[None,:,:]\n",
        "\n",
        "\n",
        "    #add 'Other' sum to transition matrix\n",
        "    transition_max = np.vstack((np.hstack((transition_max, last_col)), np.hstack((last_row, diagonal_elements))))\n",
        "      # filter to find the remaining ones\n",
        "    filter = (np.sum(np.sum(transition_max, axis=0), axis=1)) > threshold_in\n",
        "\n",
        "    trans_flt_max = transition_max[filter,:,:][:,filter,:]\n",
        "\n",
        "#filter labels list accordingnly\n",
        "labels_long_flt_max = [item for (item, cond) in zip(labels_long_filtered, filter) if cond]\n",
        "labels_short_flt_max = [item for (item, cond) in zip(labels_short_filtered, filter) if cond]\n",
        "\n",
        "#check if Other has survived this time\n",
        "if trans_flt_max.shape[0] != len(labels_long_flt_max): #'Other' label transition has survived after summation\n",
        "  labels_long_flt_max.append('Other')\n",
        "  labels_short_flt_max.append('Other')"
      ],
      "metadata": {
        "id": "cgfqd2GvWQpJ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhCD8pDbD8DL"
      },
      "source": [
        "## **Part 3: graph for filtered transitions and fixed nodes**\n",
        "\n",
        "For the visualizing nodes, we wanted to divide the graph into two parts. A part for DH labels which are ordered based on their level. The other part would be all other lables. In the next cell, I separated the labels and also sorted them alphabetically.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "LlJraDMaHwd4"
      },
      "outputs": [],
      "source": [
        "#sort the labels alphabetically and get old indices for both DH and non-DHs:\n",
        "#method 'all'\n",
        "mask = ['DH' in s for s in labels_short_flt_all]\n",
        "sorted_DH_all = sorted(((name, index) for index, name in\n",
        "                         enumerate([item for (item, cond) in zip(labels_short_flt_all, mask) if cond]))\n",
        "                          , reverse=True)\n",
        "sorted_non_DH_all = sorted((name, index) for index, name in\n",
        "                         enumerate([item for (item, cond) in zip(labels_short_flt_all, mask) if not cond]))\n",
        "\n",
        "#method 'min'\n",
        "mask = ['DH' in s for s in labels_short_flt_min]\n",
        "sorted_DH_min = sorted(((name, index) for index, name in\n",
        "                         enumerate([item for (item, cond) in zip(labels_short_flt_min, mask) if cond]))\n",
        "                          , reverse=True)\n",
        "sorted_non_DH_min = sorted((name, index) for index, name in\n",
        "                         enumerate([item for (item, cond) in zip(labels_short_flt_min, mask) if not cond]))\n",
        "\n",
        "#method 'max'\n",
        "mask = ['DH' in s for s in labels_short_flt_max]\n",
        "sorted_DH_max = sorted(((name, index) for index, name in\n",
        "                         enumerate([item for (item, cond) in zip(labels_short_flt_max, mask) if cond]))\n",
        "                          , reverse=True)\n",
        "sorted_non_DH_max = sorted((name, index) for index, name in\n",
        "                         enumerate([item for (item, cond) in zip(labels_short_flt_max, mask) if not cond]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the end, I built six (three, number of methods by two, escalated and non-escalated) graphs.\n",
        "The size of each node is proportional to the number of inward transitions. The width of each edge is proportional to the number of transition between the two nodes (except for the transitions to node 'end').\n",
        "I added a variable 'threshod' to omit the eadges with less number of transition. If any edge transition is lower than the 'threshod' the edge won't be shown in the graph.\n",
        "I also have removed the transitions from an edge to itself. It helped declutter the graph substantially. Besides, it meant that the conversation remained in a state for a longer time (which we did not care about).\n",
        "\n",
        "The graphs will be save as 'html' files in the current directory (after running next cell, open 'files' on the left menu bar; you should see two files with names \"nonesc_fixed.html\" and \"esc_fixed.html\"; the files are downloadable).\n"
      ],
      "metadata": {
        "id": "U6XkigeQ_VZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's look at the 'max' graphs:"
      ],
      "metadata": {
        "id": "hRnA6gVNaBzl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "GX4pbpQFkZW5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "outputId": "aebfaebe-915a-4fda-92a2-1e9f99059943"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nonesc_fixed_max.html\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-3c5aed581c5b>\u001b[0m in \u001b[0;36m<cell line: 205>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;31m#create and save the graphs in current directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m \u001b[0mnon_esc_net_max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nonesc_fixed_max.html\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0mesc_net_max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"esc_fixed_max.html\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyvis/network.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, name, local, notebook)\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnotebook\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen_browser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnotebook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen_browser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyvis/network.py\u001b[0m in \u001b[0;36mwrite_html\u001b[0;34m(self, name, local, notebook, open_browser)\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0mgetcwd_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0mcheck_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetcwd_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnotebook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnotebook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdn_resources\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"local\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyvis/network.py\u001b[0m in \u001b[0;36mgenerate_html\u001b[0;34m(self, name, local, notebook)\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mphysics_enabled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphysics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m         self.html = template.render(height=height,\n\u001b[0m\u001b[1;32m    480\u001b[0m                                     \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m                                     \u001b[0mnodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'render'"
          ]
        }
      ],
      "source": [
        "# create two empty directed Network instance, with certain size in pixels\n",
        "non_esc_net_max = Network(height='900px', width='900px',directed =True)\n",
        "esc_net_max = Network(height='900px', width='900px',directed =True)\n",
        "# a threshold for showing the frequency of transitions\n",
        "threshold = 12\n",
        "node_color = '#0000CC'\n",
        "esc_edge_color = '#FF9933'\n",
        "non_esc_edge_color = '#33FF33'\n",
        "\n",
        "#keep track of node index\n",
        "idx_node = 0\n",
        "# add nodes to network\n",
        "#left column for DHs & right col for the rest\n",
        "# index is the sorted index in DH or non-DH (sotrted_DH or sorted_non_DH)\n",
        "# idx is the index in the filtered label matrix (all the labels)\n",
        "# each node would be part of an ellipse for the visualization purposes\n",
        "# each group of nodes is part of a half ellipse (two different circles)\n",
        "\n",
        "\n",
        "# number of nodes in the left part of the ellipse; all DHs\n",
        "node_no_col = 7 #len(sorted_DH_max)\n",
        "# generate coordinates for all the nodes in the left part\n",
        "y = np.linspace(0,600,node_no_col)\n",
        "x = -np.sqrt(300**2 - ((y-300)**2)/1.1) - 50\n",
        "# add nodes from sorted_DH\n",
        "for node, _ in sorted_DH_max:\n",
        "  idx = labels_short_flt_max.index(node)\n",
        "\n",
        "  #check its frequency by summing the column\n",
        "  value = int(np.sum(trans_flt_max[:,idx,0]))\n",
        "\n",
        "  non_esc_net_max.add_node(idx_node, label=node, value=value*10,\n",
        "               x= int(x[idx_node]), y=int(y[idx_node]),\n",
        "               color=node_color)\n",
        "\n",
        "  value = int(np.sum(trans_flt_max[:,idx,1]))\n",
        "\n",
        "  esc_net_max.add_node(idx_node, label=node, value=value*10,\n",
        "               x= int(x[idx_node]), y=int(y[idx_node]),\n",
        "               color=node_color)\n",
        "\n",
        "  idx_node += 1\n",
        "\n",
        "# number of nodes in the left part of the circle\n",
        "node_no_col = len(sorted_non_DH_max)\n",
        "#positions of the nodes for the right part\n",
        "y = np.linspace(0,600,node_no_col)\n",
        "x = (np.sqrt(300**2 - ((y-300)**2)/1.1) + 50)\n",
        "\n",
        "#add non-DH nodes to the graph\n",
        "for node, _ in sorted_non_DH_max:\n",
        "  # idx of the node on labels_short list\n",
        "  idx = labels_short_flt_max.index(node)\n",
        "\n",
        "  #check its frequency by summing the column\n",
        "  value = int(np.sum(trans_flt_max[:,idx,0]))\n",
        "  # print(value)\n",
        "  non_esc_net_max.add_node(idx_node, label=node, value=value*10,\n",
        "               x= int(x[idx_node - len(sorted_DH_max)]),\n",
        "               y=int(y[idx_node - len(sorted_DH_max)]),\n",
        "               color=node_color)\n",
        "\n",
        "  value = int(np.sum(trans_flt_all[:,idx,1]))\n",
        "  # print(value)\n",
        "  esc_net_max.add_node(idx_node, label=node, value=value*10,\n",
        "               x= int(x[idx_node - len(sorted_DH_max)]),\n",
        "               y=int(y[idx_node - len(sorted_DH_max)]),\n",
        "               color=node_color)\n",
        "\n",
        "  idx_node += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# add edges from DHs to both\n",
        "idx_source = 0\n",
        "for source, _ in sorted_DH_max:\n",
        "  # idx of the node on labels_short list\n",
        "  idx = labels_short_flt_max.index(source)\n",
        "  # print(node, idx)\n",
        "\n",
        "\n",
        "  idx_target = 0\n",
        "  for target, _ in sorted_DH_max:\n",
        "    #remove transitions to the source itself\n",
        "    if target == source:\n",
        "      idx_target += 1\n",
        "      continue\n",
        "  # idx of the node on labels_short list\n",
        "    idx_t = labels_short_flt_max.index(target)\n",
        "    weight = int(trans_flt_max[idx,idx_t,0])\n",
        "    # weight = penfunc(trans_freq,5)\n",
        "\n",
        "    if weight > threshold:\n",
        "      non_esc_net_max.add_edge(idx_source, idx_target,\n",
        "                  weight=weight,\n",
        "                  value=weight,\n",
        "                  color=non_esc_edge_color)\n",
        "    weight = int(trans_flt_max[idx,idx_t,1])\n",
        "    # weight = penfunc(trans_freq,5)\n",
        "\n",
        "    if weight > threshold:\n",
        "      esc_net_max.add_edge(idx_source, idx_target,\n",
        "                  weight=weight,\n",
        "                  value=weight,\n",
        "                  color=esc_edge_color)\n",
        "\n",
        "\n",
        "    idx_target += 1\n",
        "  for target, _ in sorted_non_DH_max:\n",
        "    #remove transitions to the source itself\n",
        "    if target == source:\n",
        "      idx_target += 1\n",
        "      continue\n",
        "  # idx of the node on labels_short list\n",
        "    idx_t = labels_short_flt_max.index(target)\n",
        "    weight = int(trans_flt_max[idx,idx_t,0])\n",
        "    if target == 'end' and weight > 0:\n",
        "      weight = threshold + 1\n",
        "    if weight > threshold:\n",
        "      non_esc_net_max.add_edge(idx_source, idx_target,\n",
        "                  weight=weight,\n",
        "                  value=weight,\n",
        "                  color=non_esc_edge_color)\n",
        "\n",
        "    weight = int(trans_flt_max[idx,idx_t,1])\n",
        "    if target == 'end' and weight > 0:\n",
        "      weight = threshold + 1\n",
        "    if weight > threshold:\n",
        "      esc_net_max.add_edge(idx_source, idx_target,\n",
        "                  weight=weight,\n",
        "                  value=weight,\n",
        "                  color=esc_edge_color)\n",
        "\n",
        "    idx_target += 1\n",
        "  idx_source += 1\n",
        "# add edges from non_DHs to both\n",
        "for source, _ in sorted_non_DH_max:\n",
        "  # idx of the node on labels_short list\n",
        "  idx = labels_short_flt_max.index(source)\n",
        "  # print(node, idx)\n",
        "\n",
        "\n",
        "  idx_target = 0\n",
        "  for target, _ in sorted_DH_max:\n",
        "\n",
        "  # idx of the node on labels_short list\n",
        "    idx_t = labels_short_flt_max.index(target)\n",
        "    weight = int(transition_all[idx,idx_t,0])\n",
        "    if weight > threshold:\n",
        "      non_esc_net_max.add_edge(idx_source, idx_target,\n",
        "                  weight=weight,\n",
        "                  value=weight,\n",
        "                  color=non_esc_edge_color)\n",
        "\n",
        "    weight = int(trans_flt_all[idx,idx_t,1])\n",
        "    if weight > threshold:\n",
        "      esc_net_max.add_edge(idx_source, idx_target,\n",
        "                  weight=weight,\n",
        "                  value=weight,\n",
        "                  color=esc_edge_color)\n",
        "\n",
        "    idx_target += 1\n",
        "\n",
        "  for target, _ in sorted_non_DH_max:\n",
        "    #remove transitions to the source itself\n",
        "    if target == source:\n",
        "      idx_target += 1\n",
        "      continue\n",
        "  # idx of the node on labels_short list\n",
        "    idx_t = labels_short_flt_max.index(target)\n",
        "    weight = int(trans_flt_max[idx,idx_t,0])\n",
        "    # weight = penfunc(trans_freq,5)\n",
        "    if target == 'end':\n",
        "      weight = threshold + 1\n",
        "    if weight > threshold:\n",
        "      non_esc_net_max.add_edge(idx_source, idx_target,\n",
        "                  weight=weight,\n",
        "                  value=weight,\n",
        "                  color=non_esc_edge_color)\n",
        "    weight = int(trans_flt_max[idx,idx_t,1])\n",
        "    # weight = penfunc(trans_freq,5)\n",
        "    if target == 'end':\n",
        "      weight = threshold + 1\n",
        "    if weight > threshold:\n",
        "      esc_net_max.add_edge(idx_source, idx_target,\n",
        "                  weight=weight,\n",
        "                  value=weight,\n",
        "                  color=esc_edge_color)\n",
        "\n",
        "    idx_target += 1\n",
        "  idx_source += 1\n",
        "\n",
        "\n",
        "# toggle_physics method changes the position of nodes based on the strength\n",
        "# of the edges. Since we wanted to have the same positions for both graphs, this\n",
        "# option is off\n",
        "non_esc_net_max.toggle_physics(False)\n",
        "esc_net_max.toggle_physics(False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#create and save the graphs in current directory\n",
        "non_esc_net_max.show(\"nonesc_fixed_max.html\")\n",
        "esc_net_max.show(\"esc_fixed_max.html\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#show the escalated graph\n",
        "display(HTML('esc_fixed_max.html'))\n",
        "#you can drag the nodes or zoom/unzoom the plot"
      ],
      "metadata": {
        "id": "85j6SegS8-Pw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#show the non-escalated graph\n",
        "display(HTML('nonesc_fixed_max.html'))\n",
        "#you can drag the nodes or zoom/unzoom the plot"
      ],
      "metadata": {
        "id": "e6LMF6wfXqvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We could see that using less higher order 'DH' arguments is not the case for 'escalated' vs 'non-escalated' conversations. However, it seems that using higher DHs did not associate with more \"cooperative\" conversations in \"escalated\" discussions.\n",
        "Now we built the graph with 'min' method to see if lower order DHs were more present in \"escalated\" conversations."
      ],
      "metadata": {
        "id": "OWYmRAfwde65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create two empty directed Network instance, with certain size in pixels\n",
        "non_esc_net_min = Network(height='900px', width='900px',directed =True)\n",
        "esc_net_min = Network(height='900px', width='900px',directed =True)\n",
        "# a threshold for showing the frequency of transitions\n",
        "threshold = 12\n",
        "node_color = '#0000CC'\n",
        "esc_edge_color = '#FF9933'\n",
        "non_esc_edge_color = '#33FF33'\n",
        "\n",
        "#keep track of node index\n",
        "idx_node = 0\n",
        "# add nodes to network\n",
        "#left column for DHs & right col for the rest\n",
        "# index is the sorted index in DH or non-DH (sotrted_DH or sorted_non_DH)\n",
        "# idx is the index in the filtered label matrix (all the labels)\n",
        "# each node would be part of an ellipse for the visualization purposes\n",
        "# each group of nodes is part of a half ellipse (two different circles)\n",
        "\n",
        "\n",
        "# number of nodes in the left part of the ellipse; all DHs\n",
        "node_no_col = 7 #len(sorted_DH_min)\n",
        "# generate coordinates for all the nodes in the left part\n",
        "y = np.linspace(0,600,node_no_col)\n",
        "x = -np.sqrt(300**2 - ((y-300)**2)/1.1) - 50\n",
        "# add nodes from sorted_DH\n",
        "for node, _ in sorted_DH_min:\n",
        "  idx = labels_short_flt_min.index(node)\n",
        "\n",
        "  #check its frequency by summing the column\n",
        "  value = int(np.sum(trans_flt_min[:,idx,0]))\n",
        "\n",
        "  non_esc_net_min.add_node(idx_node, label=node, value=value*10,\n",
        "               x= int(x[idx_node]), y=int(y[idx_node]),\n",
        "               color=node_color)\n",
        "\n",
        "  value = int(np.sum(trans_flt_min[:,idx,1]))\n",
        "\n",
        "  esc_net_min.add_node(idx_node, label=node, value=value*10,\n",
        "               x= int(x[idx_node]), y=int(y[idx_node]),\n",
        "               color=node_color)\n",
        "\n",
        "  idx_node += 1\n",
        "\n",
        "# number of nodes in the left part of the circle\n",
        "node_no_col = len(sorted_non_DH_min)\n",
        "#positions of the nodes for the right part\n",
        "y = np.linspace(0,600,node_no_col)\n",
        "x = (np.sqrt(300**2 - ((y-300)**2)/1.1) + 50)\n",
        "\n",
        "#add non-DH nodes to the graph\n",
        "for node, _ in sorted_non_DH_min:\n",
        "  # idx of the node on labels_short list\n",
        "  idx = labels_short_flt_min.index(node)\n",
        "\n",
        "  #check its frequency by summing the column\n",
        "  value = int(np.sum(trans_flt_min[:,idx,0]))\n",
        "  # print(value)\n",
        "  non_esc_net_min.add_node(idx_node, label=node, value=value*10,\n",
        "               x= int(x[idx_node - len(sorted_DH_min)]),\n",
        "               y=int(y[idx_node - len(sorted_DH_min)]),\n",
        "               color=node_color)\n",
        "\n",
        "  value = int(np.sum(trans_flt_all[:,idx,1]))\n",
        "  # print(value)\n",
        "  esc_net_min.add_node(idx_node, label=node, value=value*10,\n",
        "               x= int(x[idx_node - len(sorted_DH_min)]),\n",
        "               y=int(y[idx_node - len(sorted_DH_min)]),\n",
        "               color=node_color)\n",
        "\n",
        "  idx_node += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# add edges from DHs to both\n",
        "idx_source = 0\n",
        "for source, _ in sorted_DH_min:\n",
        "  # idx of the node on labels_short list\n",
        "  idx = labels_short_flt_min.index(source)\n",
        "  # print(node, idx)\n",
        "\n",
        "\n",
        "  idx_target = 0\n",
        "  for target, _ in sorted_DH_min:\n",
        "    #remove transitions to the source itself\n",
        "    if target == source:\n",
        "      idx_target += 1\n",
        "      continue\n",
        "  # idx of the node on labels_short list\n",
        "    idx_t = labels_short_flt_min.index(target)\n",
        "    weight = int(trans_flt_min[idx,idx_t,0])\n",
        "    # weight = penfunc(trans_freq,5)\n",
        "\n",
        "    if weight > threshold:\n",
        "      non_esc_net_min.add_edge(idx_source, idx_target,\n",
        "                  weight=weight,\n",
        "                  value=weight,\n",
        "                  color=non_esc_edge_color)\n",
        "    weight = int(trans_flt_min[idx,idx_t,1])\n",
        "    # weight = penfunc(trans_freq,5)\n",
        "\n",
        "    if weight > threshold:\n",
        "      esc_net_min.add_edge(idx_source, idx_target,\n",
        "                  weight=weight,\n",
        "                  value=weight,\n",
        "                  color=esc_edge_color)\n",
        "\n",
        "\n",
        "    idx_target += 1\n",
        "  for target, _ in sorted_non_DH_min:\n",
        "    #remove transitions to the source itself\n",
        "    if target == source:\n",
        "      idx_target += 1\n",
        "      continue\n",
        "  # idx of the node on labels_short list\n",
        "    idx_t = labels_short_flt_min.index(target)\n",
        "    weight = int(trans_flt_min[idx,idx_t,0])\n",
        "    if target == 'end' and weight > 0:\n",
        "      weight = threshold + 1\n",
        "    if weight > threshold:\n",
        "      non_esc_net_min.add_edge(idx_source, idx_target,\n",
        "                  weight=weight,\n",
        "                  value=weight,\n",
        "                  color=non_esc_edge_color)\n",
        "\n",
        "    weight = int(trans_flt_min[idx,idx_t,1])\n",
        "    if target == 'end' and weight > 0:\n",
        "      weight = threshold + 1\n",
        "    if weight > threshold:\n",
        "      esc_net_min.add_edge(idx_source, idx_target,\n",
        "                  weight=weight,\n",
        "                  value=weight,\n",
        "                  color=esc_edge_color)\n",
        "\n",
        "    idx_target += 1\n",
        "  idx_source += 1\n",
        "# add edges from non_DHs to both\n",
        "for source, _ in sorted_non_DH_min:\n",
        "  # idx of the node on labels_short list\n",
        "  idx = labels_short_flt_min.index(source)\n",
        "  # print(node, idx)\n",
        "\n",
        "\n",
        "  idx_target = 0\n",
        "  for target, _ in sorted_DH_min:\n",
        "\n",
        "  # idx of the node on labels_short list\n",
        "    idx_t = labels_short_flt_min.index(target)\n",
        "    weight = int(transition_all[idx,idx_t,0])\n",
        "    if weight > threshold:\n",
        "      non_esc_net_min.add_edge(idx_source, idx_target,\n",
        "                  weight=weight,\n",
        "                  value=weight,\n",
        "                  color=non_esc_edge_color)\n",
        "\n",
        "    weight = int(trans_flt_all[idx,idx_t,1])\n",
        "    if weight > threshold:\n",
        "      esc_net_min.add_edge(idx_source, idx_target,\n",
        "                  weight=weight,\n",
        "                  value=weight,\n",
        "                  color=esc_edge_color)\n",
        "\n",
        "    idx_target += 1\n",
        "\n",
        "  for target, _ in sorted_non_DH_min:\n",
        "    #remove transitions to the source itself\n",
        "    if target == source:\n",
        "      idx_target += 1\n",
        "      continue\n",
        "  # idx of the node on labels_short list\n",
        "    idx_t = labels_short_flt_min.index(target)\n",
        "    weight = int(trans_flt_min[idx,idx_t,0])\n",
        "    # weight = penfunc(trans_freq,5)\n",
        "    if target == 'end':\n",
        "      weight = threshold + 1\n",
        "    if weight > threshold:\n",
        "      non_esc_net_min.add_edge(idx_source, idx_target,\n",
        "                  weight=weight,\n",
        "                  value=weight,\n",
        "                  color=non_esc_edge_color)\n",
        "    weight = int(trans_flt_min[idx,idx_t,1])\n",
        "    # weight = penfunc(trans_freq,5)\n",
        "    if target == 'end':\n",
        "      weight = threshold + 1\n",
        "    if weight > threshold:\n",
        "      esc_net_min.add_edge(idx_source, idx_target,\n",
        "                  weight=weight,\n",
        "                  value=weight,\n",
        "                  color=esc_edge_color)\n",
        "\n",
        "    idx_target += 1\n",
        "  idx_source += 1\n",
        "\n",
        "\n",
        "# toggle_physics method changes the position of nodes based on the strength\n",
        "# of the edges. Since we wanted to have the same positions for both graphs, this\n",
        "# option is off\n",
        "non_esc_net_min.toggle_physics(False)\n",
        "esc_net_min.toggle_physics(False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#create and save the graphs in current directory\n",
        "non_esc_net_min.show(\"nonesc_fixed_min.html\")\n",
        "esc_net_min.show(\"esc_fixed_min.html\")\n"
      ],
      "metadata": {
        "id": "rbW7zbF8d7gC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#show the non-escalated graph\n",
        "display(HTML('nonesc_fixed_min.html'))\n",
        "#you can drag the nodes or zoom/unzoom the plot"
      ],
      "metadata": {
        "id": "fZfIglCUeo_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#show the non-escalated graph\n",
        "display(HTML('esc_fixed_min.html'))\n",
        "#you can drag the nodes or zoom/unzoom the plot"
      ],
      "metadata": {
        "id": "T7xQ6zpyerwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems like lower order DHs were actually used more in \"escalated\" conversations. This means that although higher order DHs were used in \"escalated\" as much as \"non-escalated\" conversation, but these higher DHs were accompanied by lower DHs (especially DH1); this could be part of the reason why less coordination was seen.\n",
        "Given both graphs, we had the idea of giving all the labels equall opportunities to be seen; we built the graphs with \"all\" method."
      ],
      "metadata": {
        "id": "jPPeUsnWAR_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create two empty directed Network instance, with certain size in pixels\n",
        "non_esc_net_all = Network(height='900px', width='900px',\n",
        "                          directed =True)\n",
        "esc_net_all = Network(height='900px', width='900px',directed =True)\n",
        "# a threshold for showing the frequency of transitions\n",
        "threshold = 12\n",
        "node_color = '#0000CC'\n",
        "esc_edge_color = '#FF9933'\n",
        "non_esc_edge_color = '#33FF33'\n",
        "\n",
        "#keep track of node index\n",
        "idx_node = 0\n",
        "# add nodes to network\n",
        "#left column for DHs & right col for the rest\n",
        "# index is the sorted index in DH or non-DH (sotrted_DH or sorted_non_DH)\n",
        "# idx is the index in the filtered label matrix (all the labels)\n",
        "# each node would be part of an ellipse for the visualization purposes\n",
        "# each group of nodes is part of a half ellipse (two different circles)\n",
        "\n",
        "\n",
        "# number of nodes in the left part of the ellipse; all DHs\n",
        "node_no_col = 7 #len(sorted_DH_all)\n",
        "# generate coordinates for all the nodes in the left part\n",
        "y = np.linspace(0,600,node_no_col)\n",
        "x = -np.sqrt(300**2 - ((y-300)**2)/1.1) - 50\n",
        "# add nodes from sorted_DH\n",
        "for node, _ in sorted_DH_all:\n",
        "  idx = labels_short_flt_all.index(node)\n",
        "\n",
        "  #check its frequency by summing the column\n",
        "  value = int(np.sum(trans_flt_all[:,idx,0]))\n",
        "\n",
        "  non_esc_net_all.add_node(idx_node, label=node, value=value*10,\n",
        "               x= int(x[idx_node]), y=int(y[idx_node]),\n",
        "               color=node_color)\n",
        "\n",
        "  value = int(np.sum(trans_flt_all[:,idx,1]))\n",
        "\n",
        "  esc_net_all.add_node(idx_node, label=node, value=value*10,\n",
        "               x= int(x[idx_node]), y=int(y[idx_node]),\n",
        "               color=node_color)\n",
        "\n",
        "  idx_node += 1\n",
        "\n",
        "# number of nodes in the left part of the circle\n",
        "node_no_col = len(sorted_non_DH_all)\n",
        "#positions of the nodes for the right part\n",
        "y = np.linspace(0,600,node_no_col)\n",
        "x = (np.sqrt(300**2 - ((y-300)**2)/1.1) + 50)\n",
        "\n",
        "#add non-DH nodes to the graph\n",
        "for node, _ in sorted_non_DH_all:\n",
        "  # idx of the node on labels_short list\n",
        "  idx = labels_short_flt_all.index(node)\n",
        "\n",
        "  #check its frequency by summing the column\n",
        "  value = int(np.sum(trans_flt_all[:,idx,0]))\n",
        "  # print(value)\n",
        "  non_esc_net_all.add_node(idx_node, label=node, value=value*10,\n",
        "               x= int(x[idx_node - len(sorted_DH_all)]),\n",
        "               y=int(y[idx_node - len(sorted_DH_all)]),\n",
        "               color=node_color)\n",
        "\n",
        "  value = int(np.sum(trans_flt_all[:,idx,1]))\n",
        "  # print(value)\n",
        "  esc_net_all.add_node(idx_node, label=node, value=value*10,\n",
        "               x= int(x[idx_node - len(sorted_DH_all)]),\n",
        "               y=int(y[idx_node - len(sorted_DH_all)]),\n",
        "               color=node_color)\n",
        "\n",
        "  idx_node += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# add edges from DHs to both\n",
        "idx_source = 0\n",
        "for source, _ in sorted_DH_all:\n",
        "  # idx of the node on labels_short list\n",
        "  idx = labels_short_flt_all.index(source)\n",
        "  # print(node, idx)\n",
        "\n",
        "\n",
        "  idx_target = 0\n",
        "  for target, _ in sorted_DH_all:\n",
        "    #remove transitions to the source itself\n",
        "    if target == source:\n",
        "      idx_target += 1\n",
        "      continue\n",
        "  # idx of the node on labels_short list\n",
        "    idx_t = labels_short_flt_all.index(target)\n",
        "    weight = int(trans_flt_all[idx,idx_t,0])\n",
        "    # weight = penfunc(trans_freq,5)\n",
        "\n",
        "    if weight > threshold:\n",
        "      non_esc_net_all.add_edge(idx_source, idx_target,\n",
        "                  weight=weight,\n",
        "                  value=weight,\n",
        "                  color=non_esc_edge_color)\n",
        "    weight = int(trans_flt_all[idx,idx_t,1])\n",
        "    # weight = penfunc(trans_freq,5)\n",
        "\n",
        "    if weight > threshold:\n",
        "      esc_net_all.add_edge(idx_source, idx_target,\n",
        "                  weight=weight,\n",
        "                  value=weight,\n",
        "                  color=esc_edge_color)\n",
        "\n",
        "\n",
        "    idx_target += 1\n",
        "  for target, _ in sorted_non_DH_all:\n",
        "    #remove transitions to the source itself\n",
        "    if target == source:\n",
        "      idx_target += 1\n",
        "      continue\n",
        "  # idx of the node on labels_short list\n",
        "    idx_t = labels_short_flt_all.index(target)\n",
        "    weight = int(trans_flt_all[idx,idx_t,0])\n",
        "    if target == 'end' and weight > 0:\n",
        "      weight = threshold + 1\n",
        "    if weight > threshold:\n",
        "      non_esc_net_all.add_edge(idx_source, idx_target,\n",
        "                  weight=weight,\n",
        "                  value=weight,\n",
        "                  color=non_esc_edge_color)\n",
        "\n",
        "    weight = int(trans_flt_all[idx,idx_t,1])\n",
        "    if target == 'end' and weight > 0:\n",
        "      weight = threshold + 1\n",
        "    if weight > threshold:\n",
        "      esc_net_all.add_edge(idx_source, idx_target,\n",
        "                  weight=weight,\n",
        "                  value=weight,\n",
        "                  color=esc_edge_color)\n",
        "\n",
        "    idx_target += 1\n",
        "  idx_source += 1\n",
        "# add edges from non_DHs to both\n",
        "for source, _ in sorted_non_DH_all:\n",
        "  # idx of the node on labels_short list\n",
        "  idx = labels_short_flt_all.index(source)\n",
        "  # print(node, idx)\n",
        "\n",
        "\n",
        "  idx_target = 0\n",
        "  for target, _ in sorted_DH_all:\n",
        "\n",
        "  # idx of the node on labels_short list\n",
        "    idx_t = labels_short_flt_all.index(target)\n",
        "    weight = int(transition_all[idx,idx_t,0])\n",
        "    if weight > threshold:\n",
        "      non_esc_net_all.add_edge(idx_source, idx_target,\n",
        "                  weight=weight,\n",
        "                  value=weight,\n",
        "                  color=non_esc_edge_color)\n",
        "\n",
        "    weight = int(trans_flt_all[idx,idx_t,1])\n",
        "    if weight > threshold:\n",
        "      esc_net_all.add_edge(idx_source, idx_target,\n",
        "                  weight=weight,\n",
        "                  value=weight,\n",
        "                  color=esc_edge_color)\n",
        "\n",
        "    idx_target += 1\n",
        "\n",
        "  for target, _ in sorted_non_DH_all:\n",
        "    #remove transitions to the source itself\n",
        "    if target == source:\n",
        "      idx_target += 1\n",
        "      continue\n",
        "  # idx of the node on labels_short list\n",
        "    idx_t = labels_short_flt_all.index(target)\n",
        "    weight = int(trans_flt_all[idx,idx_t,0])\n",
        "    # weight = penfunc(trans_freq,5)\n",
        "    if target == 'end':\n",
        "      weight = threshold + 1\n",
        "    if weight > threshold:\n",
        "      non_esc_net_all.add_edge(idx_source, idx_target,\n",
        "                  weight=weight,\n",
        "                  value=weight,\n",
        "                  color=non_esc_edge_color)\n",
        "    weight = int(trans_flt_all[idx,idx_t,1])\n",
        "    # weight = penfunc(trans_freq,5)\n",
        "    if target == 'end':\n",
        "      weight = threshold + 1\n",
        "    if weight > threshold:\n",
        "      esc_net_all.add_edge(idx_source, idx_target,\n",
        "                  weight=weight,\n",
        "                  value=weight,\n",
        "                  color=esc_edge_color)\n",
        "\n",
        "    idx_target += 1\n",
        "  idx_source += 1\n",
        "\n",
        "\n",
        "# toggle_physics method changes the position of nodes based on the strength\n",
        "# of the edges. Since we wanted to have the same positions for both graphs, this\n",
        "# option is off\n",
        "non_esc_net_all.toggle_physics(False)\n",
        "esc_net_all.toggle_physics(False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#create and save the graphs in current directory\n",
        "non_esc_net_all.show(\"nonesc_fixed_all.html\")\n",
        "esc_net_all.show(\"esc_fixed_all.html\")\n"
      ],
      "metadata": {
        "id": "yM17TrH-B8jI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#show the non-escalated graph\n",
        "display(HTML('nonesc_fixed_all.html'))\n",
        "#you can drag the nodes or zoom/unzoom the plot"
      ],
      "metadata": {
        "id": "ogdxMb9kCDTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#show the non-escalated graph\n",
        "display(HTML('esc_fixed_all.html'))\n",
        "#you can drag the nodes or zoom/unzoom the plot"
      ],
      "metadata": {
        "id": "0MMQ-UJyCHmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing the left and right parts of the graphs in \"escalated\" and \"non-escalated\" conversation, using \"all\" labels shows less transitions between the left part (rebutal tactics) and right part (resolusional tactics) in the \"escalated\" graph. This means that although authors in escalated conversations used almost the same level of reasoning to discuss their points of view, but it did not encourge using more resolusional arguments such as \"Asking quesions\", \"providing a clarification\", or \"suggesting a compromise\"."
      ],
      "metadata": {
        "id": "G5zdji4okODi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='refs'></a>\n",
        "***References***:\n",
        "1. Paul Graham. 2008. *How to disagree*. http://www.paulgraham.com/disagree.html\n",
        "2. Christine De Kock, Tom Stafford, Andreas Vlachos. 2022. *How to disagree well: Investigating the dispute tactics used on Wikipedia*. Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing."
      ],
      "metadata": {
        "id": "dAPU0k5bWDYC"
      }
    }
  ]
}